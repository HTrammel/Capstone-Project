---
title: "Running Project Notes"
author: "Harold Trammel"
date: "December 9, 2015"
output: html_document
---

## Task 0 - Understanding the problem

### Tasks to accomplish

* Obtain the data (download and manipulate)
* Familiarizing yourself with NLP and text mining - Learn about the basics of natural language processing and how it relates to the data science process you have learned in the Data Science Specialization.

### Questions to consider

* What do the data look like?
	- The files we are using are large and not easily read into most editors.
	- As expected there are misspellings within the random paragraphs.
	- There are emojis, e.g. :).  Need to decide how to handle those.
	- The Russian file promises to be a pain in general. 

* Where do the data come from?

* Can you think of any other data sources that might help you in this project?
	- I will probably need a profanity list, which has to already exist

* What are the common steps in natural language processing?
	- Most discuss removing stopwords as they can be noise

* What are some common issues in the analysis of text data?
	- Punctuation (keep/remove)
	- Misspellings or odd words
	- Ambiguity in meaning

* What is the relationship between NLP and the concepts you have learned in the Specialization?
	- I have to figure out the test, validation, and training sets.
	- I need to have cross-validation that can withstand several runs


### Activities

`r Sys.Date()`: 

* Zip file downloaded
* LOCALE.blogs.txt extracted.  "de", "en", "fi", and "ru" locales were extracted.
* Watched the "welcome" videos
* Watched the "Understand the Problem" video
* Watched the "Getting and Cleaning the Data" video
* Began reading _Text Mining Infrastructure in R_
* Attempted to load and explore data files

    * used scan with the following:
    
    ```
    scan("Data/en_US.blogs.txt", what="list", nmax=100)
    ```
    
    * used readLines with the following:
    
    ``` 
    con <- file("Data/en_US.blogs.txt","r")
    readLines(con,1)
    ```
* Created the variables for all of the folders and files.

### Observations

* Not all text mining is done with R.  Note the use of basic Unix text processing tools.
* The foreign language sources will just have to wait.
* The 'tm' package is a pain.
* Need to start designing the use case for the shinyapp.

## Task 1 - Data acquisition and cleaning

Large databases comprising of text in a target language are commonly used when generating language models for various purposes. In this exercise, you will use the English database but may consider three other databases in German, Russian and Finnish.

The goal of this task is to get familiar with the databases and do the necessary cleaning. After this exercise, you should understand what real data looks like and how much effort you need to put into cleaning the data. When you commence on developing a new language, the first thing is to understand the language and its peculiarities with respect to your target. You can learn to read, speak and write the language. Alternatively, you can study data and learn from existing information about the language through literature and the internet. At the very least, you need to understand how the language is written: writing script, existing input methods, some phonetic knowledge, etc.

Make sure you have downloaded the data from Coursera before heading for the exercises. This exercise uses the files named LOCALE.blogs.txt where LOCALE is the each of the four locales en_US, de_DE, ru_RU and fi_FI. The data is from a corpus called HC Corpora (www.corpora.heliohost.org). See the readme file at http://www.corpora.heliohost.org/aboutcorpus.html for details on the corpora available. Note, download the data from the Coursera site. The files have been language filtered but may still contain some foreign text.

Note that the data contain words of offensive and profane meaning. They are left there intentionally to highlight the fact that the developer has to work on them.

#### Tasks to accomplish

__Tokenization__ - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.

__Profanity filtering__ - removing profanity and other words you do not want to predict.

#### Questions to consider

* How should you handle punctuation?
	- For our purposes, interword punctuation should be removed.
	- Intraword, e.g. contractions and possessives should be kept
	- Hashtags are a special case to be consisdered

* The data contains lots of times, dates, numbers and currency values. How to handle these? Are they useful for prediction?
* How do you find typos in the data?
	- I would hope that typos will be at the bottom of a frequency listing.

* How do you identify garbage, or the wrong language?
* How do you define profanity? How do you ensure you don't remove words you want to include?
* How do you handle capital and lower cases?
* What is the best set of features you might use to predict the next word?

#### Data acquisition and cleaning

##### Tips and hints

The data sets for this project are reasonably large. It may cause problems for your computer if you try to read the whole data set in at once. You can use the file() and readLines() functions to read a fixed number of lines into R to begin your exploration.

### Activities

### Observations


## Task 2 - Exploratory analysis
The first step in building a predictive model for text is understanding the distribution and relationship between the words, tokens, and phrases in the text. The goal of this task is to understand the basic relationships you observe in the data and prepare to build your first linguistic models.

#### Tasks to accomplish

__Exploratory analysis__ - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.

__Understand frequencies of words and word pairs__ - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

#### Questions to consider

* Some words are more frequent than others - what are the distributions of word frequencies?
* What are the frequencies of 2-grams and 3-grams in the dataset?
* How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
* How do you evaluate how many of the words come from foreign languages?
* Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

#### Exploratory analysis

##### Tips and hints

Consider how you are going to perform your basic exploratory and predictive analyses. Keep in mind that the first n rows of the data set may not be representative. You might want to think about how to sample the data using file() and readLines() to obtain a representative sample. 

Think hard about ways you can "compress" the data, what words appear frequently? What combinations of words appear frequently? Later, when you build the R model object, it will need to be small enough to upload to a Shiny server. The more you can figure out how to compress the data the smaller this object will be. 

### Activities

### Observations


## Task 3 - Modeling

The goal here is to build your first simple model for the relationship between words. This is the first step in building a predictive text mining application. You will explore simple models and discover more complicated modeling techniques.

#### Tasks to accomplish

__Build basic n-gram model__ - using the exploratory analysis you performed, build a basic n-gram model (http://en.wikipedia.org/wiki/N-gram) for predicting the next word based on the previous 1, 2, or 3 words.

__Build a model to handle unseen n-grams__ - in some cases people will want to type a combination of words that does not appear in the corpora. Build a model to handle cases where a particular n-gram isn't observed.

#### Questions to consider

* How can you efficiently store an n-gram model (think Markov Chains)?
* How can you use the knowledge about word frequencies to make your model smaller and more efficient?
* How many parameters do you need (i.e. how big is n in your n-gram model)?
* Can you think of simple ways to "smooth" the probabilities (think about giving all n-grams a non-zero probability even if they aren't observed in the data) ?
* How do you evaluate whether your model is any good?
* How can you use backoff models to estimate the probability of unobserved n-grams?

#### Modeling

##### Tips and hints

Think about starting simply, using your sampled data - what would be your first choice word prediction if you only knew the frequency of each individual word? 
How would you build the model if you were predicting using only one previous word? 

### Activities

### Observations

